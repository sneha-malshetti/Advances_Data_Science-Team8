{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230617.log\n",
      "<class 'boto.s3.bucket.Bucket'>\n",
      "b'CleanData/nj_230617_WBAN_54779_clean.csv'\n",
      "b'NJ_190617_WBAN_54779.csv'\n",
      "b'NJ_200617_WBAN_54779.csv'\n",
      "b'NJ_210617_WBAN_54779.csv'\n",
      "Number of files in S3 bucket  4\n",
      "NJ_230617_WBAN_54779.csv\n",
      "possible None\n",
      "Shape of 1st file is : (4220, 90)\n",
      "NJ_210617_WBAN_54779.csv\n",
      "(4070, 90)\n",
      "Shape of second file is : (4070, 90)\n",
      "Shape of merged file is : (4244, 90)\n",
      "Shape of merged n duplicated removed file is : (4220, 90)\n",
      "successfully uploaded to s3\n",
      "NJ_230617_WBAN_54779.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from boto.s3.connection import S3Connection\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import boto.s3\n",
    "import sys\n",
    "import datetime\n",
    "from boto.s3.key import Key\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import csv\n",
    "import io\n",
    "import requests\n",
    "import glob\n",
    "import logging\n",
    "import logging.handlers\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "#Loading 2 Json Config files\n",
    "\n",
    "with open('config.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "    \n",
    "with open('initconfig.json') as data_file1:    \n",
    "    data1 = json.load(data_file1)\n",
    "    \n",
    "# secret keys \n",
    "\n",
    "AWSAccess1=data[\"AWSAccess\"]\n",
    "AWSSecret1=data[\"AWSSecret\"]\n",
    "\n",
    "#json variables\n",
    "\n",
    "# pprint(data)\n",
    "link1=data[\"link\"]\n",
    "# print(link1)\n",
    "\n",
    "state1=data[\"state\"]\n",
    "# print(state1)\n",
    "\n",
    "stationId=data[\"StationId\"]\n",
    "    \n",
    "linkpt1= data1[\"linkpart1\"]\n",
    "linkpt2= data1[\"linkpart2\"]\n",
    "\n",
    "\n",
    "#current date time\n",
    "datestr = time.strftime(\"%d%m%Y\")\n",
    "datestr= datestr[0:4]+datestr[-2:]\n",
    "# print (datestr)\n",
    "\n",
    "#for log files datestring.\n",
    "datestr2=time.strftime(\"%d%m%Y%H%M%S\")\n",
    "datestr2= datestr2[0:4]+datestr2[-8:-6]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#log generation of files on local directory \n",
    "\n",
    "LOG_FILENAME = datestr2+'.log'\n",
    "# Set up a specific logger with our desired output level\n",
    "my_logger = logging.getLogger('MyLogger')\n",
    "\n",
    "if not my_logger.handlers:\n",
    "\n",
    "    my_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Add the log message handler to the logger\n",
    "    handler = logging.handlers.TimedRotatingFileHandler( filename= LOG_FILENAME, when= 'd', interval= 1,\n",
    "                                                    backupCount= 120)\n",
    "\n",
    "    my_logger.addHandler(handler)\n",
    " # create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s',datefmt='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# add formatter to handler\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "\n",
    "# See what files are created\n",
    "logfiles = glob.glob('%s*' % LOG_FILENAME)\n",
    "\n",
    "for filename in logfiles:\n",
    "    print (filename)\n",
    "\n",
    "\n",
    "#.............................................Log file generated..........................................................\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fname for current date files \n",
    "fname = state1+\"_\"+datestr+stationId+\".csv\"\n",
    "\n",
    "#Connection variables\n",
    "\n",
    "c = boto.connect_s3(AWSAccess1, AWSSecret1)\n",
    "conn = S3Connection(AWSAccess1, AWSSecret1)\n",
    "\n",
    "#Create create bucket on S3 if doesnt exist\n",
    "bucket = conn.create_bucket('team8njassignment1')\n",
    "#print (bucket)\n",
    "print(type(bucket))\n",
    "\n",
    "\n",
    "#Fetch all the initial data and check if data exists already\n",
    "\n",
    "\n",
    "bucket = c.get_bucket('team8njassignment1')\n",
    "count =0\n",
    "for key in bucket.list():\n",
    "    lists3files=key.name.encode('utf-8')\n",
    "    print (lists3files)\n",
    "    count=count+1\n",
    "print ('Number of files in S3 bucket ', count)\n",
    "\n",
    "\n",
    "if count ==0:\n",
    "    #init File merge \n",
    "    initfile1 = pd.read_csv(linkpt1)\n",
    "    print(\"Shape of 1st file is :\",initfile1.shape)\n",
    "#     print (initfile1.head(n=1).iloc[:, [5]],)\n",
    "#     print (initfile1.tail(n=1).iloc[:, [5]],)\n",
    "\n",
    "    initfile2 = pd.read_csv(linkpt2)\n",
    "    print(\"Shape of 1st file is :\",initfile2.shape)\n",
    "#     print (initfile2.head(n=1).iloc[:, [5]],)\n",
    "#     print (initfile2.tail(n=1).iloc[:, [5]],)\n",
    "\n",
    "\n",
    "    initfullmerge=pd.concat([initfile1,initfile2], axis=0).drop_duplicates().reset_index(drop=True)\n",
    "    print(\"Shape of merged file is :\",initfullmerge.shape)\n",
    "    initfullmerge2=initfullmerge.drop_duplicates(['DATE'], keep='first')\n",
    "    print(\"Shape of merged n duplicated removed file is :\",initfullmerge2.shape)\n",
    "#     print(initfullmerge2.head(5))\n",
    "#     print (type(initfullmerge2))\n",
    "    \n",
    "#download on local directory init data    \n",
    "    initialfilename='initfile.csv'\n",
    "    initfullmerge2.to_csv(initialfilename,sep=',', index=False)\n",
    "    # log file save event\n",
    "    my_logger.info(\"A csv file named 'initfile' was saved in the local repository at:\" +time.strftime(\"%d%m%Y%H%M%S\"))\n",
    "\n",
    "    \n",
    "#congif.Json file update last init file.\n",
    "\n",
    "\n",
    "    data[\"lastChangedFile\"]= \"initfile.csv\"\n",
    "    filename= 'config.json'\n",
    "    with open('config.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        data['lastChangedFile'] = initialfilename # <--- add `id` value.\n",
    "\n",
    "    os.remove(filename)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    #log json file change\n",
    "    my_logger.info(\"JSON file, 'config.json' was updated at:\" +time.strftime(\"%d%m%Y%H%M%S\"))\n",
    "\n",
    "    #......................Initial data created. = initfile.csv..................\n",
    "\n",
    "# upload the current date data\n",
    "\n",
    "b = c.get_bucket(bucket, validate=False)\n",
    "\n",
    "k=Key(bucket)\n",
    "k.key=fname\n",
    "possiblekey=bucket.get_key(\"Rawdata/\"+fname)\n",
    "print(fname)\n",
    "print('possible', possiblekey)\n",
    "if possiblekey==None:\n",
    "        url = link1\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "        #Todays data extracted from link\n",
    "            todaylink= pd.read_csv(link1)\n",
    "            print(\"Shape of 1st file is :\",todaylink.shape)\n",
    "        todaylink= pd.read_csv(link1)\n",
    "        #extracting 2nd(previous days) file into dataframe\n",
    "        \n",
    "        with open('config.json') as data_file:    \n",
    "                data = json.load(data_file)   \n",
    "                \n",
    "        lastchangedfile= data[\"lastChangedFile\"] \n",
    "        links3=\"https://s3.amazonaws.com/team8njassignment1/Rawdata/\"+lastchangedfile\n",
    "        print(lastchangedfile)\n",
    "        \n",
    "        r = requests.get(links3)\n",
    "        if r.status_code == 200:\n",
    "     #Todays data extracted from link\n",
    "            prevdata= pd.read_csv(links3)\n",
    "            print (prevdata.shape)\n",
    "        prevdata= pd.read_csv(links3)\n",
    "        print(\"Shape of second file is :\",prevdata.shape)\n",
    "#         prevdata= pd.read_csv(links3)   \n",
    "        #Merge previous and today\n",
    "        \n",
    "        dailymerge=pd.concat([prevdata,todaylink], axis=0).drop_duplicates().reset_index(drop=True)\n",
    "        print(\"Shape of merged file is :\",dailymerge.shape)\n",
    "        dailymerge2=dailymerge.drop_duplicates(['DATE'], keep='first')\n",
    "        print(\"Shape of merged n duplicated removed file is :\",dailymerge2.shape)\n",
    "        \n",
    "      \n",
    "        #upload the file \n",
    "               \n",
    "        k = Key(b)\n",
    "        k.key = \"Rawdata/\"+fname\n",
    "        k.content_type = r.headers['content-type']\n",
    "        k.set_contents_from_string(r.content)\n",
    "        print('successfully uploaded to s3')\n",
    "        #log upload event\n",
    "        my_logger.info(\"A file for the day was uploaded on S3 at:\" +time.strftime(\"%d%m%Y%H%M%S\"))\n",
    "\n",
    "        \n",
    "        #download current day file on local as well.\n",
    "        \n",
    "        dailymerge2.to_csv(fname,sep=',')\n",
    "#         #log upload event\n",
    "        my_logger.info(\"A file for the day was downloaded in the local repository at:\" +time.strftime(\"%d%m%Y%H%M%S\"))\n",
    "        \n",
    "        #congif.Json file daily update last changed file.\n",
    "\n",
    "        print (fname)\n",
    "        data[\"lastChangedFile\"]= fname\n",
    "        filename= 'config.json'\n",
    "        with open('config.json', 'r') as f:\n",
    "            data = json.load(f)\n",
    "            data['lastChangedFile'] = fname # <--- add `id` value.\n",
    "\n",
    "        os.remove(filename)\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "        #log json file update event \n",
    "        \n",
    "        my_logger.info(\"JSON file 'config.json was updated with the new file name :\" +time.strftime(\"%d%m%Y%H%M%S\"))\n",
    "        \n",
    "\n",
    "else:\n",
    "    #file already exist in s3 & log that event.\n",
    "    print(\"the file already exists in s3\")\n",
    "    \n",
    "    #log file already exists\n",
    "    my_logger.info(\"An attempt was made to download an already existing file at:\" +time.strftime(\"%d%m%Y%H%M%S\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
